{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#SETTINGS.PY\n",
        "\n",
        "import os\n",
        "from os.path import abspath, dirname, join\n",
        "\n",
        "\n",
        "PROJ_DIR = join(abspath('./cs145-pst'))\n",
        "DATA_DIR = join(PROJ_DIR, \"data\")\n",
        "OUT_DIR = join(PROJ_DIR, \"out\")\n",
        "\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "DATA_TRACE_DIR = DATA_DIR\n"
      ],
      "metadata": {
        "id": "t9HoH0rhnn3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#UTILS.PY\n",
        "\n",
        "from os.path import join\n",
        "import json\n",
        "import numpy as np\n",
        "import pickle\n",
        "from collections import defaultdict as dd\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')  # include timestamp\n",
        "\n",
        "\n",
        "def load_json(rfdir, rfname):\n",
        "    logger.info('loading %s ...', rfname)\n",
        "    with open(join(rfdir, rfname), 'r', encoding='utf-8') as rf:\n",
        "        data = json.load(rf)\n",
        "        logger.info('%s loaded', rfname)\n",
        "        return data\n",
        "\n",
        "\n",
        "def dump_json(obj, wfdir, wfname):\n",
        "    logger.info('dumping %s ...', wfname)\n",
        "    with open(join(wfdir, wfname), 'w', encoding='utf-8') as wf:\n",
        "        json.dump(obj, wf, indent=4, ensure_ascii=False)\n",
        "    logger.info('%s dumped.', wfname)\n",
        "\n",
        "\n",
        "def serialize_embedding(embedding):\n",
        "    return pickle.dumps(embedding)\n",
        "\n",
        "\n",
        "def deserialize_embedding(s):\n",
        "    return pickle.loads(s)\n",
        "\n",
        "\n",
        "def find_bib_context(xml, dist=100):\n",
        "    bs = BeautifulSoup(xml, \"xml\")\n",
        "    bib_to_context = dd(list)\n",
        "    bibr_strs_to_bid_id = {}\n",
        "    for item in bs.find_all(type='bibr'):\n",
        "        if \"target\" not in item.attrs:\n",
        "            continue\n",
        "        bib_id = item.attrs[\"target\"][1:]\n",
        "        item_str = \"<ref type=\\\"bibr\\\" target=\\\"{}\\\">{}</ref>\".format(item.attrs[\"target\"], item.get_text())\n",
        "        bibr_strs_to_bid_id[item_str] = bib_id\n",
        "\n",
        "    for item_str in bibr_strs_to_bid_id:\n",
        "        bib_id = bibr_strs_to_bid_id[item_str]\n",
        "        cur_bib_context_pos_start = [ii for ii in range(len(xml)) if xml.startswith(item_str, ii)]\n",
        "        for pos in cur_bib_context_pos_start:\n",
        "            bib_to_context[bib_id].append(xml[pos - dist: pos + dist].replace(\"\\n\", \" \").replace(\"\\r\", \" \").strip())\n",
        "    return bib_to_context\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "class Log:\n",
        "    def __init__(self, file_path):\n",
        "        self.file_path = file_path\n",
        "        self.f = open(file_path, 'w+')\n",
        "\n",
        "    def log(self, s):\n",
        "        self.f.write(str(datetime.now()) + \"\\t\" + s + '\\n')\n",
        "        self.f.flush()"
      ],
      "metadata": {
        "id": "PtfVkQSrnleU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XkdGK6yfKpd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from os.path import join\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict as dd\n",
        "from bs4 import BeautifulSoup\n",
        "from fuzzywuzzy import fuzz\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import BertForSequenceClassification, get_linear_schedule_with_warmup\n",
        "from transformers.optimization import AdamW\n",
        "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
        "from tqdm import trange\n",
        "from sklearn.metrics import classification_report, precision_recall_fscore_support, average_precision_score\n",
        "import logging\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
        "                    level = logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "MAX_SEQ_LENGTH=512"
      ],
      "metadata": {
        "id": "vzYyYBKqlo7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "def prepare_bert_input():\n",
        "    x_train = []\n",
        "    y_train = []\n",
        "    x_valid = []\n",
        "    y_valid = []\n",
        "    print()\n",
        "    data_dir = join(DATA_TRACE_DIR, \"PST\")\n",
        "    papers = load_json(data_dir, \"paper_source_trace_train_ans.json\")\n",
        "    n_papers = len(papers)\n",
        "    papers = sorted(papers, key=lambda x: x[\"_id\"])\n",
        "    n_train = int(n_papers * 2 / 3)\n",
        "    # n_valid = n_papers - n_train\n",
        "\n",
        "    papers_train = papers[:n_train]\n",
        "    papers_valid = papers[n_train:]\n",
        "\n",
        "    pids_train = {p[\"_id\"] for p in papers_train}\n",
        "    pids_valid = {p[\"_id\"] for p in papers_valid}\n",
        "\n",
        "    in_dir = join(data_dir, \"paper-xml\")\n",
        "    files = []\n",
        "    for f in os.listdir(in_dir):\n",
        "        if f.endswith(\".xml\"):\n",
        "            files.append(f)\n",
        "\n",
        "    pid_to_source_titles = dd(list)\n",
        "    for paper in tqdm(papers):\n",
        "        pid = paper[\"_id\"]\n",
        "        for ref in paper[\"refs_trace\"]:\n",
        "            pid_to_source_titles[pid].append(ref[\"title\"].lower())\n",
        "\n",
        "    # files = sorted(files)\n",
        "    # for file in tqdm(files):\n",
        "    for cur_pid in tqdm(pids_train | pids_valid):\n",
        "        # cur_pid = file.split(\".\")[0]\n",
        "        # if cur_pid not in pids_train and cur_pid not in pids_valid:\n",
        "            # continue\n",
        "        f = open(join(in_dir, cur_pid + \".xml\"), encoding='utf-8')\n",
        "        xml = f.read()\n",
        "        bs = BeautifulSoup(xml, \"xml\")\n",
        "\n",
        "        source_titles = pid_to_source_titles[cur_pid]\n",
        "        if len(source_titles) == 0:\n",
        "            continue\n",
        "\n",
        "        references = bs.find_all(\"biblStruct\")\n",
        "        bid_to_title = {}\n",
        "        n_refs = 0\n",
        "        for ref in references:\n",
        "            if \"xml:id\" not in ref.attrs:\n",
        "                continue\n",
        "            bid = ref.attrs[\"xml:id\"]\n",
        "            if ref.analytic is None:\n",
        "                continue\n",
        "            if ref.analytic.title is None:\n",
        "                continue\n",
        "            bid_to_title[bid] = ref.analytic.title.text.lower()\n",
        "            b_idx = int(bid[1:]) + 1\n",
        "            if b_idx > n_refs:\n",
        "                n_refs = b_idx\n",
        "\n",
        "        flag = False\n",
        "\n",
        "        cur_pos_bib = set()\n",
        "\n",
        "        for bid in bid_to_title:\n",
        "            cur_ref_title = bid_to_title[bid]\n",
        "            for label_title in source_titles:\n",
        "                if fuzz.ratio(cur_ref_title, label_title) >= 80:\n",
        "                    flag = True\n",
        "                    cur_pos_bib.add(bid)\n",
        "\n",
        "        cur_neg_bib = set(bid_to_title.keys()) - cur_pos_bib\n",
        "\n",
        "        if not flag:\n",
        "            continue\n",
        "\n",
        "        if len(cur_pos_bib) == 0 or len(cur_neg_bib) == 0:\n",
        "            continue\n",
        "\n",
        "        bib_to_contexts = find_bib_context(xml)\n",
        "\n",
        "        n_pos = len(cur_pos_bib)\n",
        "        n_neg = n_pos * 10\n",
        "        cur_neg_bib_sample = np.random.choice(list(cur_neg_bib), n_neg, replace=True)\n",
        "\n",
        "        if cur_pid in pids_train:\n",
        "            cur_x = x_train\n",
        "            cur_y = y_train\n",
        "        elif cur_pid in pids_valid:\n",
        "            cur_x = x_valid\n",
        "            cur_y = y_valid\n",
        "        else:\n",
        "            continue\n",
        "            # raise Exception(\"cur_pid not in train/valid/test\")\n",
        "\n",
        "        for bib in cur_pos_bib:\n",
        "            cur_context = \" \".join(bib_to_contexts[bib])\n",
        "            cur_x.append(cur_context)\n",
        "            cur_y.append(1)\n",
        "\n",
        "        for bib in cur_neg_bib_sample:\n",
        "            cur_context = \" \".join(bib_to_contexts[bib])\n",
        "            cur_x.append(cur_context)\n",
        "            cur_y.append(0)\n",
        "\n",
        "    print(\"len(x_train)\", len(x_train), \"len(x_valid)\", len(x_valid))\n",
        "\n",
        "\n",
        "    with open(join(data_dir, \"bib_context_train.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        for line in x_train:\n",
        "            f.write(line + \"\\n\")\n",
        "\n",
        "    with open(join(data_dir, \"bib_context_valid.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        for line in x_valid:\n",
        "            f.write(line + \"\\n\")\n",
        "\n",
        "    with open(join(data_dir, \"bib_context_train_label.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        for line in y_train:\n",
        "            f.write(str(line) + \"\\n\")\n",
        "\n",
        "    with open(join(data_dir, \"bib_context_valid_label.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        for line in y_valid:\n",
        "            f.write(str(line) + \"\\n\")\n"
      ],
      "metadata": {
        "id": "6NPqpxRXlm0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class BertInputItem(object):\n",
        "    \"\"\"An item with all the necessary attributes for finetuning BERT.\"\"\"\n",
        "\n",
        "    def __init__(self, text, input_ids, input_mask, segment_ids, label_id):\n",
        "        self.text = text\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.label_id = label_id\n",
        "\n"
      ],
      "metadata": {
        "id": "RMjUubGvluwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def convert_examples_to_inputs(example_texts, example_labels, max_seq_length, tokenizer, verbose=0):\n",
        "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
        "\n",
        "    input_items = []\n",
        "    examples = zip(example_texts, example_labels)\n",
        "    for (ex_index, (text, label)) in enumerate(examples):\n",
        "\n",
        "        # Create a list of token ids\n",
        "        input_ids = tokenizer.encode(f\"[CLS] {text} [SEP]\")\n",
        "        if len(input_ids) > max_seq_length:\n",
        "            input_ids = input_ids[:max_seq_length]\n",
        "\n",
        "        # All our tokens are in the first input segment (id 0).\n",
        "        segment_ids = [0] * len(input_ids)\n",
        "\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "        # tokens are attended to.\n",
        "        input_mask = [1] * len(input_ids)\n",
        "\n",
        "        # Zero-pad up to the sequence length.\n",
        "        padding = [0] * (max_seq_length - len(input_ids))\n",
        "        input_ids += padding\n",
        "        input_mask += padding\n",
        "        segment_ids += padding\n",
        "\n",
        "        assert len(input_ids) == max_seq_length\n",
        "        assert len(input_mask) == max_seq_length\n",
        "        assert len(segment_ids) == max_seq_length\n",
        "\n",
        "        label_id = label\n",
        "\n",
        "        input_items.append(\n",
        "            BertInputItem(text=text,\n",
        "                          input_ids=input_ids,\n",
        "                          input_mask=input_mask,\n",
        "                          segment_ids=segment_ids,\n",
        "                          label_id=label_id))\n",
        "\n",
        "    return input_items\n"
      ],
      "metadata": {
        "id": "N8YqAjJ8lxMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def get_data_loader(features, max_seq_length, batch_size, shuffle=True):\n",
        "\n",
        "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
        "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
        "    all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
        "    data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
        "\n",
        "    dataloader = DataLoader(data, shuffle=shuffle, batch_size=batch_size)\n",
        "    return dataloader\n",
        "\n"
      ],
      "metadata": {
        "id": "fVWnpatglzys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def evaluate(model, dataloader, device, criterion):\n",
        "    model.eval()\n",
        "\n",
        "    eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "    predicted_labels, correct_labels = [], []\n",
        "\n",
        "    for step, batch in enumerate(tqdm(dataloader, desc=\"Evaluation iteration\")):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        input_ids, input_mask, segment_ids, label_ids = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "            r = model(input_ids, attention_mask=input_mask,\n",
        "                                          token_type_ids=segment_ids, labels=label_ids)\n",
        "            # tmp_eval_loss = r[0]\n",
        "            logits = r[1]\n",
        "            # print(\"logits\", logits)\n",
        "            tmp_eval_loss = criterion(logits, label_ids)\n",
        "\n",
        "        outputs = np.argmax(logits.to('cpu'), axis=1)\n",
        "        label_ids = label_ids.to('cpu').numpy()\n",
        "\n",
        "        predicted_labels += list(outputs)\n",
        "        correct_labels += list(label_ids)\n",
        "\n",
        "        eval_loss += tmp_eval_loss.mean().item()\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "\n",
        "    correct_labels = np.array(correct_labels)\n",
        "    predicted_labels = np.array(predicted_labels)\n",
        "\n",
        "    return eval_loss, correct_labels, predicted_labels\n",
        "\n"
      ],
      "metadata": {
        "id": "0vmJMgIIl2M0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train(year=2023, model_name=\"scibert\"):\n",
        "    print(\"model name\", model_name)\n",
        "    train_texts = []\n",
        "    dev_texts = []\n",
        "    train_labels = []\n",
        "    dev_labels = []\n",
        "    data_year_dir = join(DATA_TRACE_DIR, \"PST\")\n",
        "    print(\"data_year_dir\", data_year_dir)\n",
        "\n",
        "    with open(join(data_year_dir, \"bib_context_train.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            train_texts.append(line.strip())\n",
        "    with open(join(data_year_dir, \"bib_context_valid.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            dev_texts.append(line.strip())\n",
        "\n",
        "    with open(join(data_year_dir, \"bib_context_train_label.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            train_labels.append(int(line.strip()))\n",
        "    with open(join(data_year_dir, \"bib_context_valid_label.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            dev_labels.append(int(line.strip()))\n",
        "\n",
        "\n",
        "    print(\"Train size:\", len(train_texts))\n",
        "    print(\"Dev size:\", len(dev_texts))\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    class_weight = len(train_labels) / (2 * np.bincount(train_labels))\n",
        "    class_weight = torch.Tensor(class_weight).to(device)\n",
        "    print(\"Class weight:\", class_weight)\n",
        "\n",
        "    if model_name == \"bert\":\n",
        "        BERT_MODEL = \"bert-base-uncased\"\n",
        "    elif model_name == \"scibert\":\n",
        "        BERT_MODEL = \"allenai/scibert_scivocab_uncased\"\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL)\n",
        "\n",
        "    model = BertForSequenceClassification.from_pretrained(BERT_MODEL, num_labels = 2)\n",
        "    model.to(device)\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss(weight=class_weight)\n",
        "\n",
        "    '''\n",
        "    ##### Sampling start\n",
        "    import random\n",
        "\n",
        "    # # Set your desired sample size\n",
        "    SAMPLE_SIZE = 100\n",
        "\n",
        "    # # Randomly select a subset of your data\n",
        "    train_texts_sample = random.sample(train_texts, SAMPLE_SIZE) # train_texts sampling instead\n",
        "    train_labels_sample = random.sample(train_labels, SAMPLE_SIZE)\n",
        "\n",
        "    train_features = convert_examples_to_inputs(train_texts_sample, train_labels_sample, MAX_SEQ_LENGTH, tokenizer, verbose=0)\n",
        "    dev_features = convert_examples_to_inputs(dev_texts, dev_labels, MAX_SEQ_LENGTH, tokenizer)\n",
        "\n",
        "    BATCH_SIZE = 16\n",
        "    train_dataloader = get_data_loader(train_features, MAX_SEQ_LENGTH, BATCH_SIZE, shuffle=True)\n",
        "    dev_dataloader = get_data_loader(dev_features, MAX_SEQ_LENGTH, BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    #### Sampling end\n",
        "    '''\n",
        "    # train_features_sample = convert_examples_to_inputs(train_texts_sample, train_labels_sample, MAX_SEQ_LENGTH, tokenizer, verbose=0)\n",
        "    # train_dataloader_sample = get_data_loader(train_features_sample, MAX_SEQ_LENGTH, BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    ### OLD CODE:\n",
        "\n",
        "    train_features = convert_examples_to_inputs(train_texts, train_labels, MAX_SEQ_LENGTH, tokenizer, verbose=0)\n",
        "    dev_features = convert_examples_to_inputs(dev_texts, dev_labels, MAX_SEQ_LENGTH, tokenizer)\n",
        "\n",
        "    BATCH_SIZE = 16\n",
        "    train_dataloader = get_data_loader(train_features, MAX_SEQ_LENGTH, BATCH_SIZE, shuffle=True)\n",
        "    dev_dataloader = get_data_loader(dev_features, MAX_SEQ_LENGTH, BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    ####\n",
        "\n",
        "    GRADIENT_ACCUMULATION_STEPS = 1\n",
        "    NUM_TRAIN_EPOCHS = 20\n",
        "    LEARNING_RATE = 5e-5\n",
        "    WARMUP_PROPORTION = 0.1\n",
        "    MAX_GRAD_NORM = 5\n",
        "\n",
        "    num_train_steps = int(len(train_dataloader.dataset) / BATCH_SIZE / GRADIENT_ACCUMULATION_STEPS * NUM_TRAIN_EPOCHS)\n",
        "    num_warmup_steps = int(WARMUP_PROPORTION * num_train_steps)\n",
        "\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE, correct_bias=False)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_train_steps)\n",
        "\n",
        "    OUTPUT_DIR = join(OUT_DIR, \"kddcup\", model_name)\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "    MODEL_FILE_NAME = \"pytorch_model.bin\"\n",
        "    PATIENCE = 5\n",
        "\n",
        "    loss_history = []\n",
        "    no_improvement = 0\n",
        "    for _ in trange(int(NUM_TRAIN_EPOCHS), desc=\"Epoch\"):\n",
        "        model.train()\n",
        "        tr_loss = 0\n",
        "        nb_tr_examples, nb_tr_steps = 0, 0\n",
        "        for step, batch in enumerate(tqdm(train_dataloader, desc=\"Training iteration\")):\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            input_ids, input_mask, segment_ids, label_ids = batch\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=input_mask, token_type_ids=segment_ids, labels=label_ids)\n",
        "            # loss = outputs[0]\n",
        "            logits = outputs[1]\n",
        "\n",
        "            loss = criterion(logits, label_ids)\n",
        "\n",
        "            if GRADIENT_ACCUMULATION_STEPS > 1:\n",
        "                loss = loss / GRADIENT_ACCUMULATION_STEPS\n",
        "\n",
        "            loss.backward()\n",
        "            tr_loss += loss.item()\n",
        "\n",
        "            if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
        "\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "                scheduler.step()\n",
        "\n",
        "        dev_loss, _, _ = evaluate(model, dev_dataloader, device, criterion)\n",
        "\n",
        "        print(\"Loss history:\", loss_history)\n",
        "        print(\"Dev loss:\", dev_loss)\n",
        "\n",
        "        if len(loss_history) == 0 or dev_loss < min(loss_history):\n",
        "            no_improvement = 0\n",
        "            model_to_save = model.module if hasattr(model, 'module') else model\n",
        "            output_model_file = os.path.join(OUTPUT_DIR, MODEL_FILE_NAME)\n",
        "            torch.save(model_to_save.state_dict(), output_model_file)\n",
        "        else:\n",
        "            no_improvement += 1\n",
        "\n",
        "        if no_improvement >= PATIENCE:\n",
        "            print(\"No improvement on development set. Finish training.\")\n",
        "            break\n",
        "\n",
        "        loss_history.append(dev_loss)\n"
      ],
      "metadata": {
        "id": "1TcOjuY0l5b3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def eval_test_papers_bert(year=2023, model_name=\"scibert\"):\n",
        "    print(\"model name\", model_name)\n",
        "    data_dir = join(DATA_TRACE_DIR, \"PST\")\n",
        "    papers_test = load_json(data_dir, \"paper_source_trace_train_ans.json\")\n",
        "    pids_test = {p[\"_id\"] for p in papers_test}\n",
        "\n",
        "    in_dir = join(data_dir, \"paper-xml\")\n",
        "    files = []\n",
        "    for f in os.listdir(in_dir):\n",
        "        cur_pid = f.split(\".\")[0]\n",
        "        if f.endswith(\".xml\") and cur_pid in pids_test:\n",
        "            files.append(f)\n",
        "\n",
        "    truths = papers_test\n",
        "    pid_to_source_titles = dd(list)\n",
        "    for paper in tqdm(truths):\n",
        "        pid = paper[\"_id\"]\n",
        "        for ref in paper[\"refs_trace\"]:\n",
        "            pid_to_source_titles[pid].append(ref[\"title\"].lower())\n",
        "\n",
        "    if model_name == \"bert\":\n",
        "        BERT_MODEL = \"bert-base-uncased\"\n",
        "    elif model_name == \"scibert\":\n",
        "        BERT_MODEL = \"allenai/scibert_scivocab_uncased\"\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"device\", device)\n",
        "    model = BertForSequenceClassification.from_pretrained(BERT_MODEL, num_labels = 2)\n",
        "    # model.load_state_dict(torch.load(join(settings.OUT_DIR, model_name, \"pytorch_model.bin\")))\n",
        "    # model.load_state_dict(torch.load(join(settings.OUT_DIR, \"bert\", \"pytorch_model.bin\")))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    BATCH_SIZE = 16\n",
        "    metrics = []\n",
        "    f_idx = 0\n",
        "\n",
        "    xml_dir = join(data_dir, \"paper-xml\")\n",
        "\n",
        "    for paper in tqdm(papers_test):\n",
        "        cur_pid = paper[\"_id\"]\n",
        "        file = join(xml_dir, cur_pid + \".xml\")\n",
        "        f = open(file, encoding='utf-8')\n",
        "\n",
        "        xml = f.read()\n",
        "        bs = BeautifulSoup(xml, \"xml\")\n",
        "        f.close()\n",
        "\n",
        "        source_titles = pid_to_source_titles[cur_pid]\n",
        "        if len(source_titles) == 0:\n",
        "            continue\n",
        "\n",
        "        references = bs.find_all(\"biblStruct\")\n",
        "        bid_to_title = {}\n",
        "        n_refs = 0\n",
        "        for ref in references:\n",
        "            if \"xml:id\" not in ref.attrs:\n",
        "                continue\n",
        "            bid = ref.attrs[\"xml:id\"]\n",
        "            if ref.analytic is None:\n",
        "                continue\n",
        "            if ref.analytic.title is None:\n",
        "                continue\n",
        "            bid_to_title[bid] = ref.analytic.title.text.lower()\n",
        "            b_idx = int(bid[1:]) + 1\n",
        "            if b_idx > n_refs:\n",
        "                n_refs = b_idx\n",
        "\n",
        "        bib_to_contexts = utils.find_bib_context(xml)\n",
        "        bib_sorted = sorted(bib_to_contexts.keys())\n",
        "\n",
        "        for bib in bib_sorted:\n",
        "            cur_bib_idx = int(bib[1:])\n",
        "            if cur_bib_idx + 1 > n_refs:\n",
        "                n_refs = cur_bib_idx + 1\n",
        "\n",
        "        y_true = [0] * n_refs\n",
        "        y_score = [0] * n_refs\n",
        "\n",
        "        flag = False\n",
        "        for bid in bid_to_title:\n",
        "            cur_ref_title = bid_to_title[bid]\n",
        "            for label_title in source_titles:\n",
        "                if fuzz.ratio(cur_ref_title, label_title) >= 80:\n",
        "                    flag = True\n",
        "                    b_idx = int(bid[1:])\n",
        "                    y_true[b_idx] = 1\n",
        "\n",
        "        if not flag:\n",
        "            continue\n",
        "\n",
        "        contexts_sorted = [\" \".join(bib_to_contexts[bib]) for bib in bib_sorted]\n",
        "\n",
        "        test_features = convert_examples_to_inputs(contexts_sorted, y_score, MAX_SEQ_LENGTH, tokenizer)\n",
        "        test_dataloader = get_data_loader(test_features, MAX_SEQ_LENGTH, BATCH_SIZE, shuffle=False)\n",
        "\n",
        "        predicted_scores = []\n",
        "        for step, batch in enumerate(test_dataloader):\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            input_ids, input_mask, segment_ids, label_ids = batch\n",
        "\n",
        "            with torch.no_grad():\n",
        "                r = model(input_ids, attention_mask=input_mask,\n",
        "                                            token_type_ids=segment_ids, labels=label_ids)\n",
        "                tmp_eval_loss = r[0]\n",
        "                logits = r[1]\n",
        "\n",
        "            cur_pred_scores = logits[:, 1].to('cpu').numpy()\n",
        "            predicted_scores.extend(cur_pred_scores)\n",
        "\n",
        "        try:\n",
        "            for ii in range(len(predicted_scores)):\n",
        "                bib_idx = int(bib_sorted[ii][1:])\n",
        "                # print(\"bib_idx\", bib_idx)\n",
        "                y_score[bib_idx] = predicted_scores[ii]\n",
        "        except IndexError as e:\n",
        "            metrics.append(0)\n",
        "            continue\n",
        "\n",
        "        cur_map = average_precision_score(y_true, y_score)\n",
        "        metrics.append(cur_map)\n",
        "        f_idx += 1\n",
        "        if f_idx % 20 == 0:\n",
        "            print(\"map until now\", np.mean(metrics), len(metrics), cur_map)\n",
        "\n",
        "    print(\"bert average map\", np.mean(metrics), len(metrics))"
      ],
      "metadata": {
        "id": "KV5Zg33EKICq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def gen_kddcup_valid_submission_bert(model_name=\"scibert\", num_votes=3):\n",
        "    print(\"model name\", model_name)\n",
        "    data_dir = join(DATA_TRACE_DIR, \"PST\")\n",
        "    papers = load_json(data_dir, \"paper_source_trace_valid_wo_ans.json\")\n",
        "\n",
        "    if model_name == \"bert\":\n",
        "        BERT_MODEL = \"bert-base-uncased\"\n",
        "    elif model_name == \"scibert\":\n",
        "        BERT_MODEL = \"allenai/scibert_scivocab_uncased\"\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL)\n",
        "\n",
        "    sub_example_dict = load_json(data_dir, \"submission_example_valid.json\")\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"device\", device)\n",
        "    model = BertForSequenceClassification.from_pretrained(BERT_MODEL, num_labels = 2)\n",
        "    model.load_state_dict(torch.load(join(OUT_DIR, \"kddcup\", model_name, \"pytorch_model.bin\")))\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    BATCH_SIZE = 16\n",
        "    # metrics = []\n",
        "    # f_idx = 0\n",
        "\n",
        "    xml_dir = join(data_dir, \"paper-xml\")\n",
        "    sub_dict = {}\n",
        "\n",
        "    for paper in tqdm(papers):\n",
        "        cur_pid = paper[\"_id\"]\n",
        "        file = join(xml_dir, cur_pid + \".xml\")\n",
        "        f = open(file, encoding='utf-8')\n",
        "        xml = f.read()\n",
        "        bs = BeautifulSoup(xml, \"xml\")\n",
        "        f.close()\n",
        "\n",
        "        references = bs.find_all(\"biblStruct\")\n",
        "        bid_to_title = {}\n",
        "        n_refs = 0\n",
        "        for ref in references:\n",
        "            if \"xml:id\" not in ref.attrs:\n",
        "                continue\n",
        "            bid = ref.attrs[\"xml:id\"]\n",
        "            if ref.analytic is None:\n",
        "                continue\n",
        "            if ref.analytic.title is None:\n",
        "                continue\n",
        "            bid_to_title[bid] = ref.analytic.title.text.lower()\n",
        "            b_idx = int(bid[1:]) + 1\n",
        "            if b_idx > n_refs:\n",
        "                n_refs = b_idx\n",
        "\n",
        "        bib_to_contexts = find_bib_context(xml)\n",
        "        # bib_sorted = sorted(bib_to_contexts.keys())\n",
        "        bib_sorted = [\"b\" + str(ii) for ii in range(n_refs)]\n",
        "\n",
        "        y_score = [0] * n_refs\n",
        "\n",
        "        assert len(sub_example_dict[cur_pid]) == n_refs\n",
        "        # continue\n",
        "\n",
        "        contexts_sorted = [\" \".join(bib_to_contexts[bib]) for bib in bib_sorted]\n",
        "\n",
        "        test_features = convert_examples_to_inputs(contexts_sorted, y_score, MAX_SEQ_LENGTH, tokenizer)\n",
        "        test_dataloader = get_data_loader(test_features, MAX_SEQ_LENGTH, BATCH_SIZE, shuffle=False)\n",
        "\n",
        "        all_predicted_scores = [] # Store predictions from multiple runs of the model\n",
        "        for _ in range(num_votes):\n",
        "            print(\"Vote: \", _) # Print current vote\n",
        "            predicted_scores = []\n",
        "            for step, batch in enumerate(test_dataloader):\n",
        "                batch = tuple(t.to(device) for t in batch)\n",
        "                input_ids, input_mask, segment_ids, label_ids = batch\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    r = model(input_ids, attention_mask=input_mask,\n",
        "                                                token_type_ids=segment_ids, labels=label_ids)\n",
        "                    tmp_eval_loss = r[0]\n",
        "                    logits = r[1]\n",
        "\n",
        "                cur_pred_scores = logits[:, 1].to('cpu').numpy()\n",
        "                predicted_scores.extend(cur_pred_scores)\n",
        "\n",
        "            all_predicted_scores.append(predicted_scores) # Add predictions to the list of multiple runs\n",
        "\n",
        "        for ii in range(len(all_predicted_scores[0])): # Just use the first predictions for the length of this loop\n",
        "            bib_idx = int(bib_sorted[ii][1:])\n",
        "            # print(\"bib_idx\", bib_idx)\n",
        "            votes = [float(sigmoid(all_predicted_scores[vote_idx][ii])) for vote_idx in range(num_votes)] # Apply the sigmoid to every prediction made\n",
        "            print(votes) # Test\n",
        "            y_score[bib_idx] = float(np.mean(votes)) # Take the mean of all the predictions, could try median too\n",
        "        sub_dict[cur_pid] = y_score\n",
        "    dump_json(sub_dict, join(OUT_DIR, \"kddcup\", model_name), \"valid_submission_scibert.json\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FHqEuEoVl_i2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    prepare_bert_input()\n",
        "    train(model_name=\"scibert\")\n",
        "    eval_test_papers_bert(model_name=\"scibert\")\n",
        "    gen_kddcup_valid_submission_bert(model_name=\"scibert\")"
      ],
      "metadata": {
        "id": "VLEnLVBemCnm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd273c51-b4e1-40fb-bab8-0804258247a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 788/788 [00:00<00:00, 553249.34it/s]\n",
            " 82%|████████▏ | 646/788 [08:43<00:58,  2.44it/s]"
          ]
        }
      ]
    }
  ]
}